{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from utils import Dataset,TESTDataset\n",
    "from model import GuideVAE,Starloss\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from decision_transformer.utils import D4RLTrajectoryDataset, evaluate_on_env, get_d4rl_normalized_score\n",
    "from decision_transformer.model import DecisionTransformer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--env', type=str, default='halfcheetah')\n",
    "parser.add_argument('--dataset', type=str, default='medium')\n",
    "parser.add_argument('--rtg_scale', type=int, default=1000)\n",
    "\n",
    "parser.add_argument('--max_eval_ep_len', type=int, default=1000)\n",
    "parser.add_argument('--num_eval_ep', type=int, default=10)\n",
    "\n",
    "parser.add_argument('--dataset_dir', type=str, default='data/')\n",
    "parser.add_argument('--log_dir', type=str, default='dt_runs/')\n",
    "\n",
    "parser.add_argument('--context_len', type=int, default=20)\n",
    "parser.add_argument('--n_blocks', type=int, default=3)\n",
    "parser.add_argument('--embed_dim', type=int, default=128)\n",
    "parser.add_argument('--n_heads', type=int, default=1)\n",
    "parser.add_argument('--dropout_p', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--wt_decay', type=float, default=1e-4)\n",
    "parser.add_argument('--warmup_steps', type=int, default=10000)\n",
    "\n",
    "parser.add_argument('--max_train_iters', type=int, default=200)\n",
    "parser.add_argument('--num_updates_per_iter', type=int, default=100)\n",
    "\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "\n",
    "parser.add_argument('--seed',type=int, default=0)\n",
    "parser.add_argument('--log_fn',type=str,default='default')\n",
    "parser.add_argument('--eval', action='store_true')\n",
    "parser.add_argument('--load_model_path', type=str,default='')\n",
    "parser.add_argument('--squences_length', type=int, default=10)\n",
    "parser.add_argument('--recovery_length', type=int, default=5)\n",
    "parser.add_argument('--total_episodes', type=int, default=2186)\n",
    "parser.add_argument('--star_batch_size', type=int, default=64)\n",
    "# parser.add_argument('--log_fn',type=str,default='default')\n",
    "parser.add_argument('--feature_size', type=int, default=240)\n",
    "parser.add_argument('--class_size', type=int, default=240)\n",
    "parser.add_argument('--latent_size', type=int, default=64)\n",
    "parser.add_argument('--batch_nums', type=int, default=1000)\n",
    "parser.add_argument('--traj_length', type=int, default=1000)\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:statesmean and statesstd=[-6.8475075e-02  1.4725347e-02 -1.8368107e-01 -2.7612743e-01\n",
      " -3.4158602e-01 -9.3233280e-02 -2.1339475e-01 -8.7635852e-02\n",
      "  5.1738744e+00 -4.2856235e-02 -3.6295928e-02  1.4089356e-01\n",
      "  6.0693521e-02  9.5642865e-02  6.7417443e-02  4.8005907e-03\n",
      "  1.2263178e-02] and [ 0.07467017  0.30077568  0.30200034  0.34436345  0.17599103  0.50729656\n",
      "  0.25660416  0.32957837  1.2546803   0.75977516  1.9806889   6.5655966\n",
      "  7.4680963   4.469275   10.567002    5.672571    7.499262  ]\n",
      "\n",
      "train:actions_mean and statesstd=[-0.32292253 -0.4103281  -0.7357532  -0.12302412 -0.46861967 -0.1577406 ] and [0.80508125 0.67032164 0.54892266 0.68336844 0.6407193  0.71960443]\n",
      "\n",
      "train:rewards_mean and statesstd=4.771085739135742 and 1.2073556648025512\n",
      "\n",
      "==================================================\n",
      "Starting new experiment:  data//halfcheetah-medium-v2.pkl\n",
      "800 trajectories, 800000 timesteps found\n",
      "Average return: 4.77, std: 1.21\n",
      "Max return: 8.33, min: -2.84\n",
      "==================================================\n",
      "test:statesmean and statesstd=[-0.06844222  0.02326619 -0.18310696 -0.2766584  -0.3405855  -0.09397404\n",
      " -0.21252804 -0.08815519  5.1699414  -0.04234011 -0.0353427   0.13914722\n",
      "  0.05954552  0.09490067  0.06715983  0.00897516  0.01799306] and [ 0.07505906  0.30939826  0.30249226  0.34356025  0.17735294  0.50695765\n",
      "  0.2571832   0.32921723  1.2699472   0.761532    1.9783132   6.569376\n",
      "  7.4624176   4.488151   10.571592    5.6718426   7.4979663 ]\n",
      "\n",
      "test:actions_mean and statesstd=[-0.32214454 -0.41149345 -0.73382115 -0.1241556  -0.46648616 -0.15889521] and [0.8056397  0.6695358  0.54962534 0.68304974 0.6414047  0.71924984]\n",
      "\n",
      "test:rewards_mean and statesstd=4.767212390899658 and 1.222189115119934\n",
      "\n",
      "==================================================\n",
      "Starting new experiment:  data//halfcheetah-medium-v2.pkl\n",
      "199 trajectories, 199000 timesteps found\n",
      "Average return: 4.77, std: 1.22\n",
      "Max return: 8.23, min: -2.71\n",
      "==================================================\n",
      "============================================================\n",
      "start time: 23-03-15-10-20-33\n",
      "============================================================\n",
      "device set to: cuda\n",
      "dataset path: data//halfcheetah-medium-v2.pkl\n",
      "model save path: dt_runs/dt_halfcheetah-medium-v2_model_23-03-15-10-20-33.pt\n",
      "log csv save path: dt_runs/dt_halfcheetah-medium-v2_log_23-03-15-10-20-33.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data_2/why_22/anaconda3/envs/safe-slac/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "seed=args.seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "dataset = args.dataset          # medium / medium-replay / medium-expert\n",
    "rtg_scale = args.rtg_scale      # normalize returns to go\n",
    "\n",
    "# use v3 env for evaluation because\n",
    "# Decision Transformer paper evaluates results on v3 envs\n",
    "\n",
    "if args.env == 'walker2d':\n",
    "    env_name = 'Walker2d-v3'\n",
    "    rtg_target = 5000\n",
    "    env_d4rl_name = f'walker2d-{dataset}-v2'\n",
    "\n",
    "elif args.env == 'halfcheetah':\n",
    "    env_name = 'HalfCheetah-v3'\n",
    "    rtg_target = 6000\n",
    "    env_d4rl_name = f'halfcheetah-{dataset}-v2'\n",
    "\n",
    "elif args.env == 'hopper':\n",
    "    env_name = 'Hopper-v3'\n",
    "    rtg_target = 3600\n",
    "    env_d4rl_name = f'hopper-{dataset}-v2'\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "max_eval_ep_len = args.max_eval_ep_len  # max len of one episode\n",
    "num_eval_ep = args.num_eval_ep          # num of evaluation episodes\n",
    "\n",
    "batch_size = args.batch_size            # training batch size\n",
    "lr = args.lr                            # learning rate\n",
    "wt_decay = args.wt_decay                # weight decay\n",
    "warmup_steps = args.warmup_steps        # warmup steps for lr scheduler\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = args.max_train_iters\n",
    "num_updates_per_iter = args.num_updates_per_iter\n",
    "\n",
    "context_len = args.context_len      # K in decision transformer\n",
    "n_blocks = args.n_blocks            # num of transformer blocks\n",
    "embed_dim = args.embed_dim          # embedding (hidden) dim of transformer\n",
    "n_heads = args.n_heads              # num of transformer heads\n",
    "dropout_p = args.dropout_p          # dropout probability\n",
    "star_batch_size = args.star_batch_size \n",
    "\n",
    "# load data from this file\n",
    "dataset_path = f'{args.dataset_dir}/{env_d4rl_name}.pkl'\n",
    "\n",
    "# saves model and csv in this directory\n",
    "log_dir = args.log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# training and evaluation device\n",
    "device = torch.device(args.device)\n",
    "traj_dataset = Dataset(dataset_path, args.context_len,False)\n",
    "traj_test_dataset = TESTDataset(dataset_path, args.context_len,False)\n",
    "traj_star_dataset = D4RLTrajectoryDataset(dataset_path, context_len, rtg_scale)\n",
    "traj_data_loader = DataLoader(\n",
    "                        traj_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True,\n",
    "                        drop_last=True\n",
    "                    )\n",
    "traj_data_test_loader = DataLoader(\n",
    "                        traj_test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True,\n",
    "                        drop_last=True\n",
    "                    )\n",
    "traj_star_loader = DataLoader(\n",
    "                        traj_star_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True,\n",
    "                        drop_last=True\n",
    "                    )\n",
    "\n",
    "data_iter = iter(traj_data_loader)\n",
    "data_test_iter = iter(traj_data_test_loader)\n",
    "star_iter = iter(traj_star_loader)\n",
    "\n",
    "state_mean, state_std, reward_mean, reward_std, action_mean, action_std = traj_star_dataset.get_state_stats()\n",
    "state_mean = torch.from_numpy(np.array(state_mean)).to(device)\n",
    "state_std = torch.from_numpy(np.array(state_std)).to(device)\n",
    "reward_mean = torch.from_numpy(np.array(reward_mean)).to(device)\n",
    "reward_std = torch.from_numpy(np.array(reward_std)).to(device)\n",
    "action_mean = torch.from_numpy(action_mean).to(device)\n",
    "action_std = torch.from_numpy(action_std).to(device)\n",
    "\n",
    "\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "prefix = \"dt_\" + env_d4rl_name\n",
    "\n",
    "save_model_name =  prefix + \"_model_\" + start_time_str + \".pt\"\n",
    "save_model_path = os.path.join(log_dir, save_model_name)\n",
    "save_best_model_path = save_model_path[:-3] + \"_best.pt\"\n",
    "\n",
    "log_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
    "log_csv_path = os.path.join(log_dir, log_csv_name)\n",
    "\n",
    "csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
    "csv_header = ([\"duration\", \"num_updates\", \"action_loss\",\n",
    "                \"eval_avg_reward\", \"eval_avg_ep_len\", \"eval_d4rl_score\"])\n",
    "\n",
    "csv_writer.writerow(csv_header)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"start time: \" + start_time_str)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"device set to: \" + str(device))\n",
    "print(\"dataset path: \" + dataset_path)\n",
    "print(\"model save path: \" + save_model_path)\n",
    "print(\"log csv save path: \" + log_csv_path)\n",
    "\n",
    "\n",
    "# ## get state stats from dataset\n",
    "# state_mean, state_std = traj_dataset.get_state_stats()\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "model_guide = GuideVAE(args.feature_size, args.class_size, args.latent_size).to(device)\n",
    "optimizer_guide = torch.optim.AdamW(\n",
    "                    model_guide.parameters(),\n",
    "                    lr=lr,\n",
    "                    weight_decay=wt_decay\n",
    "                )\n",
    "scheduler_guide = torch.optim.lr_scheduler.LambdaLR(\n",
    "                        optimizer_guide,\n",
    "                        lambda steps: min((steps+1)/warmup_steps, 1)\n",
    "                )\n",
    "\n",
    "model = DecisionTransformer(\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            n_blocks=n_blocks,\n",
    "            h_dim=embed_dim,\n",
    "            context_len=context_len,\n",
    "            n_heads=n_heads,\n",
    "            drop_p=dropout_p,\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "                    model.parameters(),\n",
    "                    lr=lr,\n",
    "                    weight_decay=wt_decay\n",
    "                )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "                        optimizer,\n",
    "                        lambda steps: min((steps+1)/warmup_steps, 1)\n",
    "                    )\n",
    "\n",
    "max_d4rl_score = -1.0\n",
    "total_updates = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of updates: 100\n",
      "GODA loss: 3.98242\n",
      "\n",
      "num of updates: 200\n",
      "GODA loss: 3.74595\n",
      "\n",
      "num of updates: 300\n",
      "GODA loss: 2.41071\n",
      "\n",
      "num of updates: 400\n",
      "GODA loss: 1.80497\n",
      "\n",
      "num of updates: 500\n",
      "GODA loss: 1.57305\n",
      "\n",
      "num of updates: 600\n",
      "GODA loss: 1.47571\n",
      "\n",
      "num of updates: 700\n",
      "GODA loss: 1.39610\n",
      "\n",
      "num of updates: 800\n",
      "GODA loss: 1.33181\n",
      "\n",
      "num of updates: 900\n",
      "GODA loss: 1.28859\n",
      "\n",
      "num of updates: 1000\n",
      "GODA loss: 1.24586\n",
      "\n",
      "num of updates: 1100\n",
      "GODA loss: 1.22213\n",
      "\n",
      "num of updates: 1200\n",
      "GODA loss: 1.19999\n",
      "\n",
      "num of updates: 1300\n",
      "GODA loss: 1.17283\n",
      "\n",
      "num of updates: 1400\n",
      "GODA loss: 1.14735\n",
      "\n",
      "num of updates: 1500\n",
      "GODA loss: 1.12033\n",
      "\n",
      "num of updates: 1600\n",
      "GODA loss: 1.09051\n",
      "\n",
      "num of updates: 1700\n",
      "GODA loss: 1.05125\n",
      "\n",
      "num of updates: 1800\n",
      "GODA loss: 1.00634\n",
      "\n",
      "num of updates: 1900\n",
      "GODA loss: 0.96757\n",
      "\n",
      "num of updates: 2000\n",
      "GODA loss: 0.91649\n",
      "\n",
      "num of updates: 2100\n",
      "GODA loss: 0.87500\n",
      "\n",
      "num of updates: 2200\n",
      "GODA loss: 0.84344\n",
      "\n",
      "num of updates: 2300\n",
      "GODA loss: 0.80925\n",
      "\n",
      "num of updates: 2400\n",
      "GODA loss: 0.78517\n",
      "\n",
      "num of updates: 2500\n",
      "GODA loss: 0.75912\n",
      "\n",
      "num of updates: 2600\n",
      "GODA loss: 0.73720\n",
      "\n",
      "num of updates: 2700\n",
      "GODA loss: 0.71699\n",
      "\n",
      "num of updates: 2800\n",
      "GODA loss: 0.70175\n",
      "\n",
      "num of updates: 2900\n",
      "GODA loss: 0.67701\n",
      "\n",
      "num of updates: 3000\n",
      "GODA loss: 0.65679\n",
      "\n",
      "num of updates: 3100\n",
      "GODA loss: 0.63441\n",
      "\n",
      "num of updates: 3200\n",
      "GODA loss: 0.61394\n",
      "\n",
      "num of updates: 3300\n",
      "GODA loss: 0.58798\n",
      "\n",
      "num of updates: 3400\n",
      "GODA loss: 0.57275\n",
      "\n",
      "num of updates: 3500\n",
      "GODA loss: 0.55169\n",
      "\n",
      "num of updates: 3600\n",
      "GODA loss: 0.52759\n",
      "\n",
      "num of updates: 3700\n",
      "GODA loss: 0.51381\n",
      "\n",
      "num of updates: 3800\n",
      "GODA loss: 0.49945\n",
      "\n",
      "num of updates: 3900\n",
      "GODA loss: 0.48447\n",
      "\n",
      "num of updates: 4000\n",
      "GODA loss: 0.45710\n",
      "\n",
      "num of updates: 4100\n",
      "GODA loss: 0.44207\n",
      "\n",
      "num of updates: 4200\n",
      "GODA loss: 0.43250\n",
      "\n",
      "num of updates: 4300\n",
      "GODA loss: 0.41468\n",
      "\n",
      "num of updates: 4400\n",
      "GODA loss: 0.39677\n",
      "\n",
      "num of updates: 4500\n",
      "GODA loss: 0.37621\n",
      "\n",
      "num of updates: 4600\n",
      "GODA loss: 0.36467\n",
      "\n",
      "num of updates: 4700\n",
      "GODA loss: 0.34909\n",
      "\n",
      "num of updates: 4800\n",
      "GODA loss: 0.33412\n",
      "\n",
      "num of updates: 4900\n",
      "GODA loss: 0.31521\n",
      "\n",
      "num of updates: 5000\n",
      "GODA loss: 0.30664\n",
      "\n",
      "num of updates: 5100\n",
      "GODA loss: 0.29141\n",
      "\n",
      "num of updates: 5200\n",
      "GODA loss: 0.27120\n",
      "\n",
      "num of updates: 5300\n",
      "GODA loss: 0.25289\n",
      "\n",
      "num of updates: 5400\n",
      "GODA loss: 0.23685\n",
      "\n",
      "num of updates: 5500\n",
      "GODA loss: 0.20982\n",
      "\n",
      "num of updates: 5600\n",
      "GODA loss: 0.20129\n",
      "\n",
      "num of updates: 5700\n",
      "GODA loss: 0.18071\n",
      "\n",
      "num of updates: 5800\n",
      "GODA loss: 0.16314\n",
      "\n",
      "num of updates: 5900\n",
      "GODA loss: 0.14364\n",
      "\n",
      "num of updates: 6000\n",
      "GODA loss: 0.12766\n",
      "\n",
      "num of updates: 6100\n",
      "GODA loss: 0.10669\n",
      "\n",
      "num of updates: 6200\n",
      "GODA loss: 0.09140\n",
      "\n",
      "num of updates: 6300\n",
      "GODA loss: 0.07488\n",
      "\n",
      "num of updates: 6400\n",
      "GODA loss: 0.05432\n",
      "\n",
      "num of updates: 6500\n",
      "GODA loss: 0.03787\n",
      "\n",
      "num of updates: 6600\n",
      "GODA loss: 0.01228\n",
      "\n",
      "num of updates: 6700\n",
      "GODA loss: -0.00258\n",
      "\n",
      "num of updates: 6800\n",
      "GODA loss: -0.02003\n",
      "\n",
      "num of updates: 6900\n",
      "GODA loss: -0.03712\n",
      "\n",
      "num of updates: 7000\n",
      "GODA loss: -0.04751\n",
      "\n",
      "num of updates: 7100\n",
      "GODA loss: -0.06991\n",
      "\n",
      "num of updates: 7200\n",
      "GODA loss: -0.08188\n",
      "\n",
      "num of updates: 7300\n",
      "GODA loss: -0.09637\n",
      "\n",
      "num of updates: 7400\n",
      "GODA loss: -0.10899\n",
      "\n",
      "num of updates: 7500\n",
      "GODA loss: -0.12801\n",
      "\n",
      "num of updates: 7600\n",
      "GODA loss: -0.14656\n",
      "\n",
      "num of updates: 7700\n",
      "GODA loss: -0.15625\n",
      "\n",
      "num of updates: 7800\n",
      "GODA loss: -0.17288\n",
      "\n",
      "num of updates: 7900\n",
      "GODA loss: -0.19379\n",
      "\n",
      "num of updates: 8000\n",
      "GODA loss: -0.20319\n",
      "\n",
      "num of updates: 8100\n",
      "GODA loss: -0.21294\n",
      "\n",
      "num of updates: 8200\n",
      "GODA loss: -0.22836\n",
      "\n",
      "num of updates: 8300\n",
      "GODA loss: -0.24865\n",
      "\n",
      "num of updates: 8400\n",
      "GODA loss: -0.26400\n",
      "\n",
      "num of updates: 8500\n",
      "GODA loss: -0.27296\n",
      "\n",
      "num of updates: 8600\n",
      "GODA loss: -0.28814\n",
      "\n",
      "num of updates: 8700\n",
      "GODA loss: -0.29597\n",
      "\n",
      "num of updates: 8800\n",
      "GODA loss: -0.30715\n",
      "\n",
      "num of updates: 8900\n",
      "GODA loss: -0.32159\n",
      "\n",
      "num of updates: 9000\n",
      "GODA loss: -0.33142\n",
      "\n",
      "num of updates: 9100\n",
      "GODA loss: -0.34336\n",
      "\n",
      "num of updates: 9200\n",
      "GODA loss: -0.34727\n",
      "\n",
      "num of updates: 9300\n",
      "GODA loss: -0.36370\n",
      "\n",
      "num of updates: 9400\n",
      "GODA loss: -0.36536\n",
      "\n",
      "num of updates: 9500\n",
      "GODA loss: -0.38436\n",
      "\n",
      "num of updates: 9600\n",
      "GODA loss: -0.39923\n",
      "\n",
      "num of updates: 9700\n",
      "GODA loss: -0.40418\n",
      "\n",
      "num of updates: 9800\n",
      "GODA loss: -0.41353\n",
      "\n",
      "num of updates: 9900\n",
      "GODA loss: -0.42162\n",
      "\n",
      "num of updates: 10000\n",
      "GODA loss: -0.42588\n",
      "\n",
      "num of updates: 10100\n",
      "GODA loss: -0.43051\n",
      "\n",
      "num of updates: 10200\n",
      "GODA loss: -0.44704\n",
      "\n",
      "num of updates: 10300\n",
      "GODA loss: -0.45758\n",
      "\n",
      "num of updates: 10400\n",
      "GODA loss: -0.46208\n",
      "\n",
      "num of updates: 10500\n",
      "GODA loss: -0.46867\n",
      "\n",
      "num of updates: 10600\n",
      "GODA loss: -0.47340\n",
      "\n",
      "num of updates: 10700\n",
      "GODA loss: -0.48170\n",
      "\n",
      "num of updates: 10800\n",
      "GODA loss: -0.49566\n",
      "\n",
      "num of updates: 10900\n",
      "GODA loss: -0.50377\n",
      "\n",
      "num of updates: 11000\n",
      "GODA loss: -0.50500\n",
      "\n",
      "num of updates: 11100\n",
      "GODA loss: -0.50845\n",
      "\n",
      "num of updates: 11200\n",
      "GODA loss: -0.51920\n",
      "\n",
      "num of updates: 11300\n",
      "GODA loss: -0.52861\n",
      "\n",
      "num of updates: 11400\n",
      "GODA loss: -0.53858\n",
      "\n",
      "num of updates: 11500\n",
      "GODA loss: -0.54718\n",
      "\n",
      "num of updates: 11600\n",
      "GODA loss: -0.54777\n",
      "\n",
      "num of updates: 11700\n",
      "GODA loss: -0.54396\n",
      "\n",
      "num of updates: 11800\n",
      "GODA loss: -0.56600\n",
      "\n",
      "num of updates: 11900\n",
      "GODA loss: -0.56003\n",
      "\n",
      "num of updates: 12000\n",
      "GODA loss: -0.57034\n",
      "\n",
      "num of updates: 12100\n",
      "GODA loss: -0.58025\n",
      "\n",
      "num of updates: 12200\n",
      "GODA loss: -0.58929\n",
      "\n",
      "num of updates: 12300\n",
      "GODA loss: -0.58367\n",
      "\n",
      "num of updates: 12400\n",
      "GODA loss: -0.59816\n",
      "\n",
      "num of updates: 12500\n",
      "GODA loss: -0.59917\n",
      "\n",
      "num of updates: 12600\n",
      "GODA loss: -0.60963\n",
      "\n",
      "num of updates: 12700\n",
      "GODA loss: -0.61205\n",
      "\n",
      "num of updates: 12800\n",
      "GODA loss: -0.62620\n",
      "\n",
      "num of updates: 12900\n",
      "GODA loss: -0.61965\n",
      "\n",
      "num of updates: 13000\n",
      "GODA loss: -0.62452\n",
      "\n",
      "num of updates: 13100\n",
      "GODA loss: -0.63550\n",
      "\n",
      "num of updates: 13200\n",
      "GODA loss: -0.63473\n",
      "\n",
      "num of updates: 13300\n",
      "GODA loss: -0.64456\n",
      "\n",
      "num of updates: 13400\n",
      "GODA loss: -0.64258\n",
      "\n",
      "num of updates: 13500\n",
      "GODA loss: -0.64975\n",
      "\n",
      "num of updates: 13600\n",
      "GODA loss: -0.65225\n",
      "\n",
      "num of updates: 13700\n",
      "GODA loss: -0.66078\n",
      "\n",
      "num of updates: 13800\n",
      "GODA loss: -0.65775\n",
      "\n",
      "num of updates: 13900\n",
      "GODA loss: -0.65898\n",
      "\n",
      "num of updates: 14000\n",
      "GODA loss: -0.65480\n",
      "\n",
      "num of updates: 14100\n",
      "GODA loss: -0.66509\n",
      "\n",
      "num of updates: 14200\n",
      "GODA loss: -0.66974\n",
      "\n",
      "num of updates: 14300\n",
      "GODA loss: -0.67908\n",
      "\n",
      "num of updates: 14400\n",
      "GODA loss: -0.67847\n",
      "\n",
      "num of updates: 14500\n",
      "GODA loss: -0.67952\n",
      "\n",
      "num of updates: 14600\n",
      "GODA loss: -0.67844\n",
      "\n",
      "num of updates: 14700\n",
      "GODA loss: -0.68940\n",
      "\n",
      "num of updates: 14800\n",
      "GODA loss: -0.69271\n",
      "\n",
      "num of updates: 14900\n",
      "GODA loss: -0.68493\n",
      "\n",
      "num of updates: 15000\n",
      "GODA loss: -0.69882\n",
      "\n",
      "num of updates: 15100\n",
      "GODA loss: -0.69767\n",
      "\n",
      "num of updates: 15200\n",
      "GODA loss: -0.70323\n",
      "\n",
      "num of updates: 15300\n",
      "GODA loss: -0.70730\n",
      "\n",
      "num of updates: 15400\n",
      "GODA loss: -0.71212\n",
      "\n",
      "num of updates: 15500\n",
      "GODA loss: -0.71098\n",
      "\n",
      "num of updates: 15600\n",
      "GODA loss: -0.71329\n",
      "\n",
      "num of updates: 15700\n",
      "GODA loss: -0.72362\n",
      "\n",
      "num of updates: 15800\n",
      "GODA loss: -0.72146\n",
      "\n",
      "num of updates: 15900\n",
      "GODA loss: -0.72208\n",
      "\n",
      "num of updates: 16000\n",
      "GODA loss: -0.73081\n",
      "\n",
      "num of updates: 16100\n",
      "GODA loss: -0.72635\n",
      "\n",
      "num of updates: 16200\n",
      "GODA loss: -0.72671\n",
      "\n",
      "num of updates: 16300\n",
      "GODA loss: -0.73602\n",
      "\n",
      "num of updates: 16400\n",
      "GODA loss: -0.73626\n",
      "\n",
      "num of updates: 16500\n",
      "GODA loss: -0.73017\n",
      "\n",
      "num of updates: 16600\n",
      "GODA loss: -0.73711\n",
      "\n",
      "num of updates: 16700\n",
      "GODA loss: -0.73675\n",
      "\n",
      "num of updates: 16800\n",
      "GODA loss: -0.74205\n",
      "\n",
      "num of updates: 16900\n",
      "GODA loss: -0.74117\n",
      "\n",
      "num of updates: 17000\n",
      "GODA loss: -0.74205\n",
      "\n",
      "num of updates: 17100\n",
      "GODA loss: -0.75079\n",
      "\n",
      "num of updates: 17200\n",
      "GODA loss: -0.74599\n",
      "\n",
      "num of updates: 17300\n",
      "GODA loss: -0.75954\n",
      "\n",
      "num of updates: 17400\n",
      "GODA loss: -0.75781\n",
      "\n",
      "num of updates: 17500\n",
      "GODA loss: -0.75530\n",
      "\n",
      "num of updates: 17600\n",
      "GODA loss: -0.75203\n",
      "\n",
      "num of updates: 17700\n",
      "GODA loss: -0.75716\n",
      "\n",
      "num of updates: 17800\n",
      "GODA loss: -0.76194\n",
      "\n",
      "num of updates: 17900\n",
      "GODA loss: -0.76005\n",
      "\n",
      "num of updates: 18000\n",
      "GODA loss: -0.76703\n",
      "\n",
      "num of updates: 18100\n",
      "GODA loss: -0.76590\n",
      "\n",
      "num of updates: 18200\n",
      "GODA loss: -0.77225\n",
      "\n",
      "num of updates: 18300\n",
      "GODA loss: -0.77089\n",
      "\n",
      "num of updates: 18400\n",
      "GODA loss: -0.76981\n",
      "\n",
      "num of updates: 18500\n",
      "GODA loss: -0.77421\n",
      "\n",
      "num of updates: 18600\n",
      "GODA loss: -0.77858\n",
      "\n",
      "num of updates: 18700\n",
      "GODA loss: -0.77100\n",
      "\n",
      "num of updates: 18800\n",
      "GODA loss: -0.77685\n",
      "\n",
      "num of updates: 18900\n",
      "GODA loss: -0.76830\n",
      "\n",
      "num of updates: 19000\n",
      "GODA loss: -0.77832\n",
      "\n",
      "num of updates: 19100\n",
      "GODA loss: -0.78450\n",
      "\n",
      "num of updates: 19200\n",
      "GODA loss: -0.78424\n",
      "\n",
      "num of updates: 19300\n",
      "GODA loss: -0.77406\n",
      "\n",
      "num of updates: 19400\n",
      "GODA loss: -0.78038\n",
      "\n",
      "num of updates: 19500\n",
      "GODA loss: -0.78475\n",
      "\n",
      "num of updates: 19600\n",
      "GODA loss: -0.78985\n",
      "\n",
      "num of updates: 19700\n",
      "GODA loss: -0.79190\n",
      "\n",
      "num of updates: 19800\n",
      "GODA loss: -0.78473\n",
      "\n",
      "num of updates: 19900\n",
      "GODA loss: -0.79082\n",
      "\n",
      "num of updates: 20000\n",
      "GODA loss: -0.78892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#先训好一个z*model：\n",
    "for i_iter in range(max_train_iters):\n",
    "    log_GODA_losses = []\n",
    "    if not args.eval:\n",
    "        for _ in range(num_updates_per_iter):\n",
    "            try:\n",
    "                states, actions, rewards = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(traj_data_loader)\n",
    "                states, actions, rewards = next(data_iter)\n",
    "\n",
    "            states = states.reshape(64,-1).to(device)          # B x T x state_dim     [64,340]\n",
    "            actions = actions.reshape(64,-1).to(device)        # B x T x act_dim       [64,120]\n",
    "            rewards = rewards.reshape(64,-1).to(device)        #                       [64,20]       \n",
    "            \n",
    "            #此处需要将三个变量合并起来\n",
    "            feature = torch.cat([states[:,0:int(state_dim*(context_len/2))],\n",
    "                                    actions[:,0:int(act_dim*(context_len/2))],\n",
    "                                    rewards[:,0:int((context_len/2))]],dim=1) #s1a1r1~s5a5r5     #[64,240]\n",
    "            feature_class = torch.cat([states[:,int(state_dim*(context_len/2)):state_dim*context_len],\n",
    "                                        actions[:,int(act_dim*(context_len/2)):act_dim*context_len],\n",
    "                                        rewards[:,int((context_len/2)):context_len]],dim=1) #s6a6r6~s10a10r10      [64,240]                                     #\n",
    "\n",
    "            recon_mu, recon_std, z1_mu, z1_log_std = model_guide.forward(feature, feature_class)\n",
    "            z2_mu, z2_log_std, recon_mu2, recon_log_std2 = model_guide.reconstruct(feature)\n",
    "            GODA_loss = model_guide.loss_function(recon_mu, recon_std, feature_class, z1_mu, z1_log_std, z2_mu, z2_log_std)\n",
    "\n",
    "            optimizer_guide.zero_grad()\n",
    "            GODA_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model_guide.parameters(), 0.25)\n",
    "            optimizer_guide.step()\n",
    "            scheduler_guide.step()\n",
    "\n",
    "            log_GODA_losses.append(GODA_loss.detach().cpu().item())\n",
    "    mean_GODA_loss = np.mean(log_GODA_losses)\n",
    "    total_updates += num_updates_per_iter\n",
    "    log_str = (\n",
    "        \"num of updates: \" + str(total_updates) + '\\n' +\n",
    "        \"GODA loss: \" +  format(mean_GODA_loss, \".5f\") + '\\n'\n",
    "    )\n",
    "    print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GODA test loss: -0.78881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_mean = state_mean.cpu().detach().numpy()\n",
    "state_std = state_std.cpu().detach().numpy()\n",
    "final_GODA_loss=[]\n",
    "for _ in range(num_updates_per_iter):\n",
    "    try:\n",
    "        states, actions, rewards = next(data_test_iter)\n",
    "    except StopIteration:\n",
    "        data_test_iter = iter(traj_data_test_loader)\n",
    "        states, actions, rewards = next(data_test_iter)\n",
    "\n",
    "    states = states.reshape(64,-1).to(device)          # B x T x state_dim     [64,170]\n",
    "    actions = actions.reshape(64,-1).to(device)        # B x T x act_dim       [64,60]\n",
    "    rewards = rewards.reshape(64,-1).to(device)  \n",
    "\n",
    "    feature = torch.cat([states[:,0:int(state_dim*(context_len/2))],\n",
    "                                actions[:,0:int(act_dim*(context_len/2))],\n",
    "                                rewards[:,0:int((context_len/2))]],dim=1) #s1a1r1~s5a5r5     \n",
    "                                                                    #\n",
    "    feature_class = torch.cat([states[:,int(state_dim*(context_len/2)):state_dim*context_len],\n",
    "                                actions[:,int(act_dim*(context_len/2)):act_dim*context_len],\n",
    "                                rewards[:,int((context_len/2)):context_len]],dim=1) #s6a6r6~s10a10r10                                                 #\n",
    "        \n",
    "    recon_mu, recon_std, z1_mu, z1_log_std = model_guide.forward(feature, feature_class)\n",
    "    z2_mu, z2_log_std, recon_mu2, recon_log_std2 = model_guide.reconstruct(feature)\n",
    "    GODA_loss = model_guide.loss_function(recon_mu, recon_std, feature_class, z1_mu, z1_log_std, z2_mu, z2_log_std)\n",
    "\n",
    "    final_GODA_loss.append(GODA_loss.detach().cpu().item())\n",
    "mean_GODA_final_loss = np.mean(final_GODA_loss)\n",
    "log_str = (\n",
    "        \"GODA test loss: \" +  format(mean_GODA_final_loss, \".5f\") + '\\n'\n",
    "    )\n",
    "print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "time elapsed: 0:06:29\n",
      "num of updates: 20100\n",
      "action loss: 0.70347\n",
      "eval avg reward: -187.27391\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:07:31\n",
      "num of updates: 20200\n",
      "action loss: 0.61637\n",
      "eval avg reward: -136.42219\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:08:31\n",
      "num of updates: 20300\n",
      "action loss: 0.51500\n",
      "eval avg reward: -117.13513\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:09:33\n",
      "num of updates: 20400\n",
      "action loss: 0.42512\n",
      "eval avg reward: -105.70835\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:10:34\n",
      "num of updates: 20500\n",
      "action loss: 0.35175\n",
      "eval avg reward: -52.74909\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:11:37\n",
      "num of updates: 20600\n",
      "action loss: 0.29756\n",
      "eval avg reward: 70.77414\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:12:39\n",
      "num of updates: 20700\n",
      "action loss: 0.26196\n",
      "eval avg reward: 170.74869\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:13:41\n",
      "num of updates: 20800\n",
      "action loss: 0.24444\n",
      "eval avg reward: 235.68205\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:14:41\n",
      "num of updates: 20900\n",
      "action loss: 0.23006\n",
      "eval avg reward: 265.58223\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:15:42\n",
      "num of updates: 21000\n",
      "action loss: 0.22113\n",
      "eval avg reward: 370.63046\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:16:48\n",
      "num of updates: 21100\n",
      "action loss: 0.21305\n",
      "eval avg reward: 402.26722\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:17:52\n",
      "num of updates: 21200\n",
      "action loss: 0.20435\n",
      "eval avg reward: 700.61933\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:18:58\n",
      "num of updates: 21300\n",
      "action loss: 0.19804\n",
      "eval avg reward: 768.92701\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:20:00\n",
      "num of updates: 21400\n",
      "action loss: 0.19179\n",
      "eval avg reward: 693.99167\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:21:06\n",
      "num of updates: 21500\n",
      "action loss: 0.18624\n",
      "eval avg reward: 512.46630\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:22:11\n",
      "num of updates: 21600\n",
      "action loss: 0.17869\n",
      "eval avg reward: 251.38748\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:23:15\n",
      "num of updates: 21700\n",
      "action loss: 0.17299\n",
      "eval avg reward: 164.33543\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:24:20\n",
      "num of updates: 21800\n",
      "action loss: 0.16871\n",
      "eval avg reward: 445.44317\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:25:25\n",
      "num of updates: 21900\n",
      "action loss: 0.16238\n",
      "eval avg reward: 385.90410\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:26:29\n",
      "num of updates: 22000\n",
      "action loss: 0.15805\n",
      "eval avg reward: 478.38377\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:27:35\n",
      "num of updates: 22100\n",
      "action loss: 0.15608\n",
      "eval avg reward: 289.19879\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:28:39\n",
      "num of updates: 22200\n",
      "action loss: 0.15206\n",
      "eval avg reward: 183.44426\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:29:46\n",
      "num of updates: 22300\n",
      "action loss: 0.14761\n",
      "eval avg reward: 246.11284\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:30:52\n",
      "num of updates: 22400\n",
      "action loss: 0.14554\n",
      "eval avg reward: 199.38469\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:31:56\n",
      "num of updates: 22500\n",
      "action loss: 0.14135\n",
      "eval avg reward: 126.70099\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:33:01\n",
      "num of updates: 22600\n",
      "action loss: 0.13822\n",
      "eval avg reward: 54.88662\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:34:05\n",
      "num of updates: 22700\n",
      "action loss: 0.13523\n",
      "eval avg reward: 99.11881\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:35:10\n",
      "num of updates: 22800\n",
      "action loss: 0.13255\n",
      "eval avg reward: 48.69240\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:36:15\n",
      "num of updates: 22900\n",
      "action loss: 0.13170\n",
      "eval avg reward: 35.64101\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:37:21\n",
      "num of updates: 23000\n",
      "action loss: 0.12794\n",
      "eval avg reward: 24.79157\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:38:27\n",
      "num of updates: 23100\n",
      "action loss: 0.12545\n",
      "eval avg reward: 24.84032\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:39:33\n",
      "num of updates: 23200\n",
      "action loss: 0.12409\n",
      "eval avg reward: -11.89005\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:40:37\n",
      "num of updates: 23300\n",
      "action loss: 0.12207\n",
      "eval avg reward: -8.83739\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:41:44\n",
      "num of updates: 23400\n",
      "action loss: 0.12060\n",
      "eval avg reward: -23.97472\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:42:49\n",
      "num of updates: 23500\n",
      "action loss: 0.11802\n",
      "eval avg reward: -21.77594\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:43:54\n",
      "num of updates: 23600\n",
      "action loss: 0.11613\n",
      "eval avg reward: -26.34505\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:45:00\n",
      "num of updates: 23700\n",
      "action loss: 0.11430\n",
      "eval avg reward: -28.64114\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:46:03\n",
      "num of updates: 23800\n",
      "action loss: 0.11241\n",
      "eval avg reward: -25.42341\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:47:09\n",
      "num of updates: 23900\n",
      "action loss: 0.11089\n",
      "eval avg reward: -32.13503\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:48:13\n",
      "num of updates: 24000\n",
      "action loss: 0.10953\n",
      "eval avg reward: -13.41146\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:49:18\n",
      "num of updates: 24100\n",
      "action loss: 0.10805\n",
      "eval avg reward: -28.24443\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:50:22\n",
      "num of updates: 24200\n",
      "action loss: 0.10650\n",
      "eval avg reward: 229.55632\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:51:29\n",
      "num of updates: 24300\n",
      "action loss: 0.10460\n",
      "eval avg reward: 12.88716\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:52:34\n",
      "num of updates: 24400\n",
      "action loss: 0.10321\n",
      "eval avg reward: -34.55884\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:53:38\n",
      "num of updates: 24500\n",
      "action loss: 0.10131\n",
      "eval avg reward: -9.46692\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:54:42\n",
      "num of updates: 24600\n",
      "action loss: 0.10114\n",
      "eval avg reward: 280.41456\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:55:46\n",
      "num of updates: 24700\n",
      "action loss: 0.09880\n",
      "eval avg reward: -4.85911\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:56:49\n",
      "num of updates: 24800\n",
      "action loss: 0.09681\n",
      "eval avg reward: -40.24121\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:57:53\n",
      "num of updates: 24900\n",
      "action loss: 0.09633\n",
      "eval avg reward: 322.56421\n",
      "\n",
      "============================================================\n",
      "time elapsed: 0:58:58\n",
      "num of updates: 25000\n",
      "action loss: 0.09409\n",
      "eval avg reward: 167.25155\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:00:03\n",
      "num of updates: 25100\n",
      "action loss: 0.09339\n",
      "eval avg reward: -17.15459\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:01:07\n",
      "num of updates: 25200\n",
      "action loss: 0.09271\n",
      "eval avg reward: -13.84049\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:02:13\n",
      "num of updates: 25300\n",
      "action loss: 0.09065\n",
      "eval avg reward: -36.89842\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:03:17\n",
      "num of updates: 25400\n",
      "action loss: 0.08961\n",
      "eval avg reward: -38.58937\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:04:22\n",
      "num of updates: 25500\n",
      "action loss: 0.08865\n",
      "eval avg reward: -40.11185\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:05:27\n",
      "num of updates: 25600\n",
      "action loss: 0.08726\n",
      "eval avg reward: -49.67187\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:06:33\n",
      "num of updates: 25700\n",
      "action loss: 0.08632\n",
      "eval avg reward: -54.80399\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:07:38\n",
      "num of updates: 25800\n",
      "action loss: 0.08609\n",
      "eval avg reward: -48.99788\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:08:43\n",
      "num of updates: 25900\n",
      "action loss: 0.08454\n",
      "eval avg reward: -36.16589\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:09:49\n",
      "num of updates: 26000\n",
      "action loss: 0.08358\n",
      "eval avg reward: -44.10061\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:10:53\n",
      "num of updates: 26100\n",
      "action loss: 0.08236\n",
      "eval avg reward: -62.11887\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:11:57\n",
      "num of updates: 26200\n",
      "action loss: 0.08206\n",
      "eval avg reward: -49.63289\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:13:03\n",
      "num of updates: 26300\n",
      "action loss: 0.08130\n",
      "eval avg reward: -38.84321\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:14:08\n",
      "num of updates: 26400\n",
      "action loss: 0.07977\n",
      "eval avg reward: -31.60493\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:15:10\n",
      "num of updates: 26500\n",
      "action loss: 0.07962\n",
      "eval avg reward: -44.49643\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:16:15\n",
      "num of updates: 26600\n",
      "action loss: 0.07855\n",
      "eval avg reward: -19.55959\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:17:18\n",
      "num of updates: 26700\n",
      "action loss: 0.07740\n",
      "eval avg reward: -30.75310\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:18:22\n",
      "num of updates: 26800\n",
      "action loss: 0.07651\n",
      "eval avg reward: -47.90618\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:19:27\n",
      "num of updates: 26900\n",
      "action loss: 0.07548\n",
      "eval avg reward: -58.49053\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:20:30\n",
      "num of updates: 27000\n",
      "action loss: 0.07481\n",
      "eval avg reward: 49.96216\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:21:35\n",
      "num of updates: 27100\n",
      "action loss: 0.07437\n",
      "eval avg reward: -52.47044\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:22:39\n",
      "num of updates: 27200\n",
      "action loss: 0.07413\n",
      "eval avg reward: -59.95364\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:23:43\n",
      "num of updates: 27300\n",
      "action loss: 0.07359\n",
      "eval avg reward: -43.70136\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:24:48\n",
      "num of updates: 27400\n",
      "action loss: 0.07261\n",
      "eval avg reward: -52.34713\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:25:53\n",
      "num of updates: 27500\n",
      "action loss: 0.07198\n",
      "eval avg reward: -41.66261\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:26:57\n",
      "num of updates: 27600\n",
      "action loss: 0.07147\n",
      "eval avg reward: -48.13897\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:28:01\n",
      "num of updates: 27700\n",
      "action loss: 0.07044\n",
      "eval avg reward: -40.66466\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:29:06\n",
      "num of updates: 27800\n",
      "action loss: 0.07083\n",
      "eval avg reward: -42.36916\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:30:09\n",
      "num of updates: 27900\n",
      "action loss: 0.06920\n",
      "eval avg reward: 416.84835\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:31:16\n",
      "num of updates: 28000\n",
      "action loss: 0.06957\n",
      "eval avg reward: -37.52743\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:32:20\n",
      "num of updates: 28100\n",
      "action loss: 0.06837\n",
      "eval avg reward: -35.10781\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:33:24\n",
      "num of updates: 28200\n",
      "action loss: 0.06825\n",
      "eval avg reward: -15.94290\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:34:28\n",
      "num of updates: 28300\n",
      "action loss: 0.06854\n",
      "eval avg reward: -36.68355\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:35:32\n",
      "num of updates: 28400\n",
      "action loss: 0.06677\n",
      "eval avg reward: 283.58976\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:36:38\n",
      "num of updates: 28500\n",
      "action loss: 0.06562\n",
      "eval avg reward: 30.97972\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:37:42\n",
      "num of updates: 28600\n",
      "action loss: 0.06645\n",
      "eval avg reward: -14.67590\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:38:47\n",
      "num of updates: 28700\n",
      "action loss: 0.06573\n",
      "eval avg reward: -6.53413\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:39:53\n",
      "num of updates: 28800\n",
      "action loss: 0.06509\n",
      "eval avg reward: 467.17362\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:40:57\n",
      "num of updates: 28900\n",
      "action loss: 0.06520\n",
      "eval avg reward: 3384.86944\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:42:00\n",
      "num of updates: 29000\n",
      "action loss: 0.06442\n",
      "eval avg reward: 28.22512\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:43:02\n",
      "num of updates: 29100\n",
      "action loss: 0.06473\n",
      "eval avg reward: 3123.28109\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:44:03\n",
      "num of updates: 29200\n",
      "action loss: 0.06425\n",
      "eval avg reward: 476.90735\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:45:05\n",
      "num of updates: 29300\n",
      "action loss: 0.06371\n",
      "eval avg reward: 2858.88217\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:46:06\n",
      "num of updates: 29400\n",
      "action loss: 0.06215\n",
      "eval avg reward: 496.14265\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:47:05\n",
      "num of updates: 29500\n",
      "action loss: 0.06242\n",
      "eval avg reward: 3524.46272\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:48:06\n",
      "num of updates: 29600\n",
      "action loss: 0.06236\n",
      "eval avg reward: 1010.10564\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:49:07\n",
      "num of updates: 29700\n",
      "action loss: 0.06141\n",
      "eval avg reward: 4177.88983\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:50:08\n",
      "num of updates: 29800\n",
      "action loss: 0.06143\n",
      "eval avg reward: 4543.89900\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:51:10\n",
      "num of updates: 29900\n",
      "action loss: 0.06089\n",
      "eval avg reward: 2328.44630\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:52:13\n",
      "num of updates: 30000\n",
      "action loss: 0.06065\n",
      "eval avg reward: 3027.12746\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:53:15\n",
      "num of updates: 30100\n",
      "action loss: 0.06028\n",
      "eval avg reward: 3481.94934\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:54:16\n",
      "num of updates: 30200\n",
      "action loss: 0.05976\n",
      "eval avg reward: 4723.53635\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:55:16\n",
      "num of updates: 30300\n",
      "action loss: 0.05919\n",
      "eval avg reward: 3965.47750\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:56:18\n",
      "num of updates: 30400\n",
      "action loss: 0.05902\n",
      "eval avg reward: 3759.57285\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:57:20\n",
      "num of updates: 30500\n",
      "action loss: 0.05845\n",
      "eval avg reward: 4636.02270\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:58:21\n",
      "num of updates: 30600\n",
      "action loss: 0.05849\n",
      "eval avg reward: 4700.29953\n",
      "\n",
      "============================================================\n",
      "time elapsed: 1:59:25\n",
      "num of updates: 30700\n",
      "action loss: 0.05818\n",
      "eval avg reward: 4465.39479\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:00:28\n",
      "num of updates: 30800\n",
      "action loss: 0.05666\n",
      "eval avg reward: 4310.87484\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:01:28\n",
      "num of updates: 30900\n",
      "action loss: 0.05773\n",
      "eval avg reward: 4738.71730\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:02:29\n",
      "num of updates: 31000\n",
      "action loss: 0.05718\n",
      "eval avg reward: 4619.55077\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:03:31\n",
      "num of updates: 31100\n",
      "action loss: 0.05757\n",
      "eval avg reward: 4505.75132\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:04:33\n",
      "num of updates: 31200\n",
      "action loss: 0.05646\n",
      "eval avg reward: 4270.62719\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:05:34\n",
      "num of updates: 31300\n",
      "action loss: 0.05656\n",
      "eval avg reward: 3203.02295\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:06:35\n",
      "num of updates: 31400\n",
      "action loss: 0.05575\n",
      "eval avg reward: 4429.01379\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:07:35\n",
      "num of updates: 31500\n",
      "action loss: 0.05487\n",
      "eval avg reward: 4887.98003\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:08:38\n",
      "num of updates: 31600\n",
      "action loss: 0.05541\n",
      "eval avg reward: 4671.36079\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:09:38\n",
      "num of updates: 31700\n",
      "action loss: 0.05560\n",
      "eval avg reward: 4663.98301\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:10:40\n",
      "num of updates: 31800\n",
      "action loss: 0.05534\n",
      "eval avg reward: 4918.72887\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:11:41\n",
      "num of updates: 31900\n",
      "action loss: 0.05551\n",
      "eval avg reward: 4516.53708\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:12:42\n",
      "num of updates: 32000\n",
      "action loss: 0.05476\n",
      "eval avg reward: 4750.48064\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:13:43\n",
      "num of updates: 32100\n",
      "action loss: 0.05451\n",
      "eval avg reward: 4967.78799\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:14:43\n",
      "num of updates: 32200\n",
      "action loss: 0.05458\n",
      "eval avg reward: 4233.79558\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:15:43\n",
      "num of updates: 32300\n",
      "action loss: 0.05436\n",
      "eval avg reward: 4990.50259\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:16:43\n",
      "num of updates: 32400\n",
      "action loss: 0.05367\n",
      "eval avg reward: 4322.29675\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:17:45\n",
      "num of updates: 32500\n",
      "action loss: 0.05374\n",
      "eval avg reward: 4426.40974\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:18:46\n",
      "num of updates: 32600\n",
      "action loss: 0.05338\n",
      "eval avg reward: 4435.89230\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:19:48\n",
      "num of updates: 32700\n",
      "action loss: 0.05274\n",
      "eval avg reward: 4680.24468\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:20:48\n",
      "num of updates: 32800\n",
      "action loss: 0.05278\n",
      "eval avg reward: 4945.34441\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:21:50\n",
      "num of updates: 32900\n",
      "action loss: 0.05240\n",
      "eval avg reward: 4992.77733\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:22:52\n",
      "num of updates: 33000\n",
      "action loss: 0.05224\n",
      "eval avg reward: 4664.01710\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:23:51\n",
      "num of updates: 33100\n",
      "action loss: 0.05268\n",
      "eval avg reward: 4643.07677\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:24:53\n",
      "num of updates: 33200\n",
      "action loss: 0.05248\n",
      "eval avg reward: 4823.28135\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:25:53\n",
      "num of updates: 33300\n",
      "action loss: 0.05169\n",
      "eval avg reward: 4729.79802\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:26:54\n",
      "num of updates: 33400\n",
      "action loss: 0.05220\n",
      "eval avg reward: 4950.76210\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:27:53\n",
      "num of updates: 33500\n",
      "action loss: 0.05217\n",
      "eval avg reward: 4952.63252\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:28:54\n",
      "num of updates: 33600\n",
      "action loss: 0.05175\n",
      "eval avg reward: 4946.50018\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:29:57\n",
      "num of updates: 33700\n",
      "action loss: 0.05119\n",
      "eval avg reward: 5016.48786\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:30:57\n",
      "num of updates: 33800\n",
      "action loss: 0.05189\n",
      "eval avg reward: 4997.06659\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:31:58\n",
      "num of updates: 33900\n",
      "action loss: 0.05111\n",
      "eval avg reward: 5032.82355\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:32:58\n",
      "num of updates: 34000\n",
      "action loss: 0.05160\n",
      "eval avg reward: 4599.93519\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:33:58\n",
      "num of updates: 34100\n",
      "action loss: 0.05105\n",
      "eval avg reward: 4715.79836\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:35:00\n",
      "num of updates: 34200\n",
      "action loss: 0.05034\n",
      "eval avg reward: 5036.04136\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:36:00\n",
      "num of updates: 34300\n",
      "action loss: 0.05068\n",
      "eval avg reward: 5032.61232\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:37:02\n",
      "num of updates: 34400\n",
      "action loss: 0.05135\n",
      "eval avg reward: 4977.08785\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:38:03\n",
      "num of updates: 34500\n",
      "action loss: 0.05071\n",
      "eval avg reward: 4963.85637\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:39:02\n",
      "num of updates: 34600\n",
      "action loss: 0.05037\n",
      "eval avg reward: 4957.59723\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:40:04\n",
      "num of updates: 34700\n",
      "action loss: 0.04994\n",
      "eval avg reward: 4916.47150\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:41:06\n",
      "num of updates: 34800\n",
      "action loss: 0.05009\n",
      "eval avg reward: 4994.06604\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:42:06\n",
      "num of updates: 34900\n",
      "action loss: 0.04985\n",
      "eval avg reward: 4995.61849\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:43:07\n",
      "num of updates: 35000\n",
      "action loss: 0.04971\n",
      "eval avg reward: 5090.51788\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:44:07\n",
      "num of updates: 35100\n",
      "action loss: 0.04933\n",
      "eval avg reward: 4986.08260\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:45:08\n",
      "num of updates: 35200\n",
      "action loss: 0.04979\n",
      "eval avg reward: 5026.23028\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:46:07\n",
      "num of updates: 35300\n",
      "action loss: 0.04935\n",
      "eval avg reward: 5005.83693\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:47:09\n",
      "num of updates: 35400\n",
      "action loss: 0.04913\n",
      "eval avg reward: 4977.25774\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:48:10\n",
      "num of updates: 35500\n",
      "action loss: 0.04904\n",
      "eval avg reward: 4539.29917\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:49:11\n",
      "num of updates: 35600\n",
      "action loss: 0.04921\n",
      "eval avg reward: 4984.55145\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:50:10\n",
      "num of updates: 35700\n",
      "action loss: 0.04883\n",
      "eval avg reward: 4938.49907\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:51:10\n",
      "num of updates: 35800\n",
      "action loss: 0.04946\n",
      "eval avg reward: 5037.78504\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:52:10\n",
      "num of updates: 35900\n",
      "action loss: 0.04875\n",
      "eval avg reward: 4962.14615\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:53:11\n",
      "num of updates: 36000\n",
      "action loss: 0.04857\n",
      "eval avg reward: 5066.68909\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:54:11\n",
      "num of updates: 36100\n",
      "action loss: 0.04824\n",
      "eval avg reward: 4922.24466\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:55:12\n",
      "num of updates: 36200\n",
      "action loss: 0.04799\n",
      "eval avg reward: 4852.05019\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:56:12\n",
      "num of updates: 36300\n",
      "action loss: 0.04804\n",
      "eval avg reward: 4963.74889\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:57:15\n",
      "num of updates: 36400\n",
      "action loss: 0.04782\n",
      "eval avg reward: 5032.92466\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:58:17\n",
      "num of updates: 36500\n",
      "action loss: 0.04821\n",
      "eval avg reward: 5058.52219\n",
      "\n",
      "============================================================\n",
      "time elapsed: 2:59:17\n",
      "num of updates: 36600\n",
      "action loss: 0.04755\n",
      "eval avg reward: 5025.63347\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:00:17\n",
      "num of updates: 36700\n",
      "action loss: 0.04810\n",
      "eval avg reward: 4889.38357\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:01:17\n",
      "num of updates: 36800\n",
      "action loss: 0.04757\n",
      "eval avg reward: 4985.93809\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:02:19\n",
      "num of updates: 36900\n",
      "action loss: 0.04756\n",
      "eval avg reward: 4741.50671\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:03:20\n",
      "num of updates: 37000\n",
      "action loss: 0.04750\n",
      "eval avg reward: 4925.53615\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:04:20\n",
      "num of updates: 37100\n",
      "action loss: 0.04824\n",
      "eval avg reward: 5049.92890\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:05:22\n",
      "num of updates: 37200\n",
      "action loss: 0.04771\n",
      "eval avg reward: 5022.31912\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:06:24\n",
      "num of updates: 37300\n",
      "action loss: 0.04735\n",
      "eval avg reward: 5030.87657\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:07:25\n",
      "num of updates: 37400\n",
      "action loss: 0.04698\n",
      "eval avg reward: 4407.96233\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:08:25\n",
      "num of updates: 37500\n",
      "action loss: 0.04681\n",
      "eval avg reward: 5006.99520\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:09:25\n",
      "num of updates: 37600\n",
      "action loss: 0.04716\n",
      "eval avg reward: 4992.00719\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:10:26\n",
      "num of updates: 37700\n",
      "action loss: 0.04632\n",
      "eval avg reward: 4995.11473\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:11:28\n",
      "num of updates: 37800\n",
      "action loss: 0.04687\n",
      "eval avg reward: 5050.47941\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:12:27\n",
      "num of updates: 37900\n",
      "action loss: 0.04651\n",
      "eval avg reward: 4850.40346\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:13:32\n",
      "num of updates: 38000\n",
      "action loss: 0.04632\n",
      "eval avg reward: 5030.37506\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:14:34\n",
      "num of updates: 38100\n",
      "action loss: 0.04597\n",
      "eval avg reward: 5117.26150\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:15:35\n",
      "num of updates: 38200\n",
      "action loss: 0.04589\n",
      "eval avg reward: 4991.21957\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:16:47\n",
      "num of updates: 38300\n",
      "action loss: 0.04732\n",
      "eval avg reward: 5017.41292\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:17:49\n",
      "num of updates: 38400\n",
      "action loss: 0.04627\n",
      "eval avg reward: 5019.63832\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:18:51\n",
      "num of updates: 38500\n",
      "action loss: 0.04633\n",
      "eval avg reward: 4978.46025\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:19:52\n",
      "num of updates: 38600\n",
      "action loss: 0.04662\n",
      "eval avg reward: 5020.19704\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:20:54\n",
      "num of updates: 38700\n",
      "action loss: 0.04626\n",
      "eval avg reward: 4991.76654\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:21:57\n",
      "num of updates: 38800\n",
      "action loss: 0.04623\n",
      "eval avg reward: 4589.89366\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:22:58\n",
      "num of updates: 38900\n",
      "action loss: 0.04548\n",
      "eval avg reward: 4900.48386\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:23:59\n",
      "num of updates: 39000\n",
      "action loss: 0.04619\n",
      "eval avg reward: 5037.91687\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:25:02\n",
      "num of updates: 39100\n",
      "action loss: 0.04520\n",
      "eval avg reward: 4988.41204\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:26:04\n",
      "num of updates: 39200\n",
      "action loss: 0.04575\n",
      "eval avg reward: 5058.16681\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:27:05\n",
      "num of updates: 39300\n",
      "action loss: 0.04508\n",
      "eval avg reward: 4980.60188\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:28:08\n",
      "num of updates: 39400\n",
      "action loss: 0.04558\n",
      "eval avg reward: 5017.77637\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:29:10\n",
      "num of updates: 39500\n",
      "action loss: 0.04598\n",
      "eval avg reward: 4931.38848\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:30:12\n",
      "num of updates: 39600\n",
      "action loss: 0.04549\n",
      "eval avg reward: 5084.00833\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:31:12\n",
      "num of updates: 39700\n",
      "action loss: 0.04502\n",
      "eval avg reward: 4955.18414\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:32:14\n",
      "num of updates: 39800\n",
      "action loss: 0.04509\n",
      "eval avg reward: 5027.11570\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:33:16\n",
      "num of updates: 39900\n",
      "action loss: 0.04540\n",
      "eval avg reward: 4958.83256\n",
      "\n",
      "============================================================\n",
      "time elapsed: 3:34:17\n",
      "num of updates: 40000\n",
      "action loss: 0.04505\n",
      "eval avg reward: 4958.42856\n",
      "\n",
      "============================================================\n",
      "finished training!\n",
      "============================================================\n",
      "started training at: 23-03-15-10-20-33\n",
      "finished training at: 23-03-15-13-54-50\n",
      "total training time: 3:34:17\n",
      "max d4rl score: 43.47453\n",
      "saved max d4rl score model at: dt_runs/dt_halfcheetah-medium-v2_model_23-03-15-10-20-33_best.pt\n",
      "saved last updated model at: dt_runs/dt_halfcheetah-medium-v2_model_23-03-15-10-20-33.pt\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def goback(rtg, r, r_, r_mean, r_std, rtg_scale):\n",
    "    r = r*r_std+r_mean\n",
    "    r_ = r_*r_std+r_mean\n",
    "    rtg = rtg*rtg_scale\n",
    "    rtg_ = np.zeros_like(rtg)\n",
    "    rtg = rtg.to(torch.device('cuda:0'))\n",
    "    rtg_ = torch.from_numpy(rtg_).to(torch.device('cuda:0'))\n",
    "    for i in range(20, 10, -1):\n",
    "        temp = torch.zeros(64).to(torch.device('cuda:0'))\n",
    "        for j in range(i - 11, 10):\n",
    "            temp += r_[:,j] - r[:,j]\n",
    "        rtg_[:,i-1] = rtg[:,i-1] + temp\n",
    "    return rtg_[:,10:20]/rtg_scale\n",
    "\n",
    "for i_train_iter in range(max_train_iters):\n",
    "\n",
    "    log_action_losses = []\n",
    "    model.train()\n",
    "\n",
    "    for _ in range(num_updates_per_iter):\n",
    "        try:\n",
    "            timesteps, states, actions, returns_to_go, traj_mask, rewards = next(star_iter)\n",
    "        except StopIteration:\n",
    "            star_iter = iter(traj_star_loader)\n",
    "            timesteps, states, actions, returns_to_go, traj_mask, rewards = next(star_iter)\n",
    "\n",
    "        #将采样出的数据输入增强网络********************************************************************************************************************************\n",
    "        states = states.reshape(star_batch_size ,-1).to(device)          # B x T x state_dim     [64,170]                                                       # \n",
    "        actions = actions.reshape(star_batch_size ,-1).to(device)        # B x T x act_dim       [64,60]                                                        #\n",
    "        rewards = rewards.reshape(star_batch_size ,-1).to(device)        #                       [64,10]                                                        #\n",
    "                                                                                                                                                                \n",
    "        feature = torch.cat([states[:,0:int(state_dim*(context_len/2))],  \n",
    "                                actions[:,0:int(act_dim*(context_len/2))],                                                                                      #\n",
    "                                    rewards[:,0:int((context_len/2))]],dim=1) #s1a1r1~s5a5r5                                                                       #    \n",
    "\n",
    "        feature_class = torch.cat([states[:,int(state_dim*(context_len/2)):state_dim*context_len],                                                              #\n",
    "                                    actions[:,int(act_dim*(context_len/2)):act_dim*context_len],                                                                #\n",
    "                                    rewards[:,int((context_len/2)):context_len]],dim=1) #s6a6r6~s10a10r10                                                       #\n",
    "                                                                                                                                                                \n",
    "        z1_mu, z1_log_std = model_guide.encode(feature, feature_class, True)                                                                                    #\n",
    "        z2_mu, z2_log_std = model_guide.encode(feature, 0, False)                                                                                               #\n",
    "        z2_log_std = torch.clamp(z2_log_std, -20, 2)                                                                                                            #\n",
    "        z2_std = torch.exp(z2_log_std)                                                                                                                          #\n",
    "\n",
    "        # recon_mu_ori,recon_log_std_ori = model_guide.decode(z1_mu,feature)                                                                                    #\n",
    "        z = z1_mu.clone().detach()                                                                                                                              #\n",
    "        z.requires_grad = True                                                                                                                                  #\n",
    "        scene_optim = torch.optim.Adam([z], lr=lr)                                                                                                              #\n",
    "        loss_star_function = Starloss()                                                                                                                         #\n",
    "        loss =[]                                                                                                                                                #\n",
    "        \n",
    "        #找到这64条序列对应的最好的z                                                                                                                              #                \n",
    "        for i in range(200):                                                                                                                                    #\n",
    "            recon_mu,recon_log_std = model_guide.decode(z,feature)                                                                                              #\n",
    "            rewards_ = recon_mu[:,(state_dim+act_dim)*int(context_len/2):]#rewards: s6'~s10'\n",
    "            rewards_ =  torch.mean(rewards_.reshape(64,10,1)*reward_std + reward_mean)                                                        #\n",
    "            loss_star = loss_star_function.forward(rewards_, z, z1_mu, z2_mu, z2_std)                                                                           #\n",
    "            loss.append(loss_star.detach().cpu().item())                                                                                                        #\n",
    "            scene_optim.zero_grad()                                                                                                                             #                 \n",
    "            loss_star.backward(retain_graph=True)                                                                                                               #\n",
    "            scene_optim.step()                                                                                                                                  #\n",
    "        \n",
    "    #替换数据                                                                                                                                                    #\n",
    "        #恢复s6'a6'r6'~s10'a10'r10'                                                                                                                             #\n",
    "        timesteps = timesteps.to(device)    # B x T                                                                                                             #\n",
    "        states_ = recon_mu[:,0:int(state_dim*(context_len/2))]                                                                                               #\n",
    "        actions_ = recon_mu[:,state_dim*int(context_len/2):(state_dim+act_dim)*int(context_len/2)].cpu().detach().numpy()\n",
    "        # actions_ =  (actions_.reshape(64,10,6)*action_std+action_mean).reshape(64,-1)\n",
    "        rewards_ = recon_mu[:,(state_dim+act_dim)*int(context_len/2):]\n",
    "        returns_to_go_ = goback(    #[64,10]\n",
    "            returns_to_go,   #[64,20]\n",
    "            rewards[:,int((context_len/2)):context_len],   #[64,10]\n",
    "            rewards_,   #[64,10]\n",
    "            reward_mean, \n",
    "            reward_std,\n",
    "            rtg_scale).to(device)  \n",
    "        \n",
    "        #拼凑1~5+6'~10'\n",
    "        states = torch.cat([states[:,0:int(state_dim*(context_len/2))],\n",
    "                            states_]).reshape(star_batch_size, context_len, state_dim).reshape(64,20,17).to(device)\n",
    "\n",
    "        actions = (torch.cat([actions[:,0:int(act_dim*(context_len/2))],\n",
    "                                torch.from_numpy(actions_).to(device)]\n",
    "                                ).reshape(star_batch_size, context_len, act_dim)*action_std+action_mean).to(device) \n",
    "        \n",
    "        # returns_to_go = torch.cat([returns_to_go[:,0:int((context_len/2))].to(device),\n",
    "        #                             returns_to_go_]).reshape(star_batch_size , context_len, 1).to(device)  \n",
    "                    \n",
    "        traj_mask = traj_mask.to(device)    # B x T                                                                                                             #\n",
    "        action_target = torch.clone(actions).detach().to(device)                                                                                                #\n",
    "        #********************************************************************************************************************************************************\n",
    "        #用star数据去训练\n",
    "        state_preds, action_preds, return_preds = model.forward(\n",
    "                                                        timesteps=timesteps,\n",
    "                                                        states=states,\n",
    "                                                        actions=actions,\n",
    "                                                        returns_to_go=returns_to_go.to(device).unsqueeze(dim = -1)\n",
    "                                                    )\n",
    "        # only consider non padded elements\n",
    "        action_preds = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
    "        action_target = action_target.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
    "\n",
    "        action_loss = F.mse_loss(action_preds, action_target, reduction='mean')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        log_action_losses.append(action_loss.detach().cpu().item())\n",
    "\n",
    "    # evaluate action accuracy\n",
    "    results = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
    "                            num_eval_ep, max_eval_ep_len, state_mean, state_std)\n",
    "\n",
    "    eval_avg_reward = results['eval/avg_reward']\n",
    "    eval_avg_ep_len = results['eval/avg_ep_len']\n",
    "    eval_d4rl_score = get_d4rl_normalized_score(results['eval/avg_reward'], env_name) * 100\n",
    "\n",
    "    mean_action_loss = np.mean(log_action_losses)\n",
    "    time_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
    "\n",
    "    total_updates += num_updates_per_iter\n",
    "\n",
    "    log_str = (\"=\" * 60 + '\\n' +\n",
    "            \"time elapsed: \" + time_elapsed  + '\\n' +\n",
    "            \"num of updates: \" + str(total_updates) + '\\n' +\n",
    "            \"action loss: \" +  format(mean_action_loss, \".5f\") + '\\n' +\n",
    "            \"eval avg reward: \" + format(eval_avg_reward, \".5f\") + '\\n' \n",
    "            # \"eval avg ep len: \" + format(eval_avg_ep_len, \".5f\") + '\\n' +\n",
    "            # \"eval d4rl score: \" + format(eval_d4rl_score, \".5f\")\n",
    "        )\n",
    "\n",
    "    print(log_str)\n",
    "\n",
    "    log_data = [time_elapsed, total_updates, mean_action_loss,\n",
    "                eval_avg_reward, eval_avg_ep_len,\n",
    "                eval_d4rl_score]\n",
    "\n",
    "    csv_writer.writerow(log_data)\n",
    "\n",
    "    # save model\n",
    "    # print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "    if eval_d4rl_score >= max_d4rl_score:\n",
    "        # print(\"saving max d4rl score model at: \" + save_best_model_path)\n",
    "        torch.save(model.state_dict(), save_best_model_path)\n",
    "        max_d4rl_score = eval_d4rl_score\n",
    "\n",
    "    # print(\"saving current model at: \" + save_model_path)\n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"finished training!\")\n",
    "print(\"=\" * 60)\n",
    "end_time = datetime.now().replace(microsecond=0) \n",
    "time_elapsed = str(end_time - start_time)\n",
    "end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "print(\"started training at: \" + start_time_str)\n",
    "print(\"finished training at: \" + end_time_str)\n",
    "print(\"total training time: \" + time_elapsed)\n",
    "print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "print(\"saved max d4rl score model at: \" + save_best_model_path)\n",
    "print(\"saved last updated model at: \" + save_model_path)\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safe-slac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
