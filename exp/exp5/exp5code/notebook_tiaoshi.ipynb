{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from utils import Dataset,TESTDataset\n",
    "from model import GuideVAE,Starloss\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from decision_transformer.utils import D4RLTrajectoryDataset, evaluate_on_env, get_d4rl_normalized_score\n",
    "from decision_transformer.model import DecisionTransformer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--env', type=str, default='halfcheetah')\n",
    "parser.add_argument('--dataset', type=str, default='medium')\n",
    "parser.add_argument('--rtg_scale', type=int, default=1000)\n",
    "\n",
    "parser.add_argument('--max_eval_ep_len', type=int, default=1000)\n",
    "parser.add_argument('--num_eval_ep', type=int, default=10)\n",
    "\n",
    "parser.add_argument('--dataset_dir', type=str, default='data/')\n",
    "parser.add_argument('--log_dir', type=str, default='dt_runs/')\n",
    "\n",
    "parser.add_argument('--context_len', type=int, default=20)\n",
    "parser.add_argument('--n_blocks', type=int, default=3)\n",
    "parser.add_argument('--embed_dim', type=int, default=128)\n",
    "parser.add_argument('--n_heads', type=int, default=1)\n",
    "parser.add_argument('--dropout_p', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--wt_decay', type=float, default=1e-4)\n",
    "parser.add_argument('--warmup_steps', type=int, default=10000)\n",
    "\n",
    "parser.add_argument('--max_train_iters', type=int, default=200)\n",
    "parser.add_argument('--num_updates_per_iter', type=int, default=100)\n",
    "\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "\n",
    "parser.add_argument('--seed',type=int, default=0)\n",
    "parser.add_argument('--log_fn',type=str,default='default')\n",
    "parser.add_argument('--eval', action='store_true')\n",
    "parser.add_argument('--load_model_path', type=str,default='')\n",
    "parser.add_argument('--squences_length', type=int, default=10)\n",
    "parser.add_argument('--recovery_length', type=int, default=5)\n",
    "parser.add_argument('--total_episodes', type=int, default=2186)\n",
    "parser.add_argument('--star_batch_size', type=int, default=64)\n",
    "# parser.add_argument('--log_fn',type=str,default='default')\n",
    "parser.add_argument('--feature_size', type=int, default=240)\n",
    "parser.add_argument('--class_size', type=int, default=240)\n",
    "parser.add_argument('--latent_size', type=int, default=64)\n",
    "parser.add_argument('--batch_nums', type=int, default=1000)\n",
    "parser.add_argument('--traj_length', type=int, default=1000)\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:statesmean and statesstd=[-6.8475075e-02  1.4725347e-02 -1.8368107e-01 -2.7612743e-01\n",
      " -3.4158602e-01 -9.3233280e-02 -2.1339475e-01 -8.7635852e-02\n",
      "  5.1738744e+00 -4.2856235e-02 -3.6295928e-02  1.4089356e-01\n",
      "  6.0693521e-02  9.5642865e-02  6.7417443e-02  4.8005907e-03\n",
      "  1.2263178e-02] and [ 0.07467017  0.30077568  0.30200034  0.34436345  0.17599103  0.50729656\n",
      "  0.25660416  0.32957837  1.2546803   0.75977516  1.9806889   6.5655966\n",
      "  7.4680963   4.469275   10.567002    5.672571    7.499262  ]\n",
      "\n",
      "train:actions_mean and statesstd=[-0.32292253 -0.4103281  -0.7357532  -0.12302412 -0.46861967 -0.1577406 ] and [0.80508125 0.67032164 0.54892266 0.68336844 0.6407193  0.71960443]\n",
      "\n",
      "train:rewards_mean and statesstd=4.771085739135742 and 1.2073556648025512\n",
      "\n",
      "==================================================\n",
      "Starting new experiment:  data//halfcheetah-medium-v2.pkl\n",
      "800 trajectories, 800000 timesteps found\n",
      "Average return: 4.77, std: 1.21\n",
      "Max return: 8.33, min: -2.84\n",
      "==================================================\n",
      "test:statesmean and statesstd=[-0.06844222  0.02326619 -0.18310696 -0.2766584  -0.3405855  -0.09397404\n",
      " -0.21252804 -0.08815519  5.1699414  -0.04234011 -0.0353427   0.13914722\n",
      "  0.05954552  0.09490067  0.06715983  0.00897516  0.01799306] and [ 0.07505906  0.30939826  0.30249226  0.34356025  0.17735294  0.50695765\n",
      "  0.2571832   0.32921723  1.2699472   0.761532    1.9783132   6.569376\n",
      "  7.4624176   4.488151   10.571592    5.6718426   7.4979663 ]\n",
      "\n",
      "test:actions_mean and statesstd=[-0.32214454 -0.41149345 -0.73382115 -0.1241556  -0.46648616 -0.15889521] and [0.8056397  0.6695358  0.54962534 0.68304974 0.6414047  0.71924984]\n",
      "\n",
      "test:rewards_mean and statesstd=4.767212390899658 and 1.222189115119934\n",
      "\n",
      "==================================================\n",
      "Starting new experiment:  data//halfcheetah-medium-v2.pkl\n",
      "199 trajectories, 199000 timesteps found\n",
      "Average return: 4.77, std: 1.22\n",
      "Max return: 8.23, min: -2.71\n",
      "==================================================\n",
      "============================================================\n",
      "start time: 23-03-13-14-34-44\n",
      "============================================================\n",
      "device set to: cuda\n",
      "dataset path: data//halfcheetah-medium-v2.pkl\n",
      "model save path: dt_runs/dt_halfcheetah-medium-v2_model_23-03-13-14-34-44.pt\n",
      "log csv save path: dt_runs/dt_halfcheetah-medium-v2_log_23-03-13-14-34-44.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data_2/why_22/anaconda3/envs/safe-slac/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "seed=args.seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "dataset = args.dataset          # medium / medium-replay / medium-expert\n",
    "rtg_scale = args.rtg_scale      # normalize returns to go\n",
    "\n",
    "# use v3 env for evaluation because\n",
    "# Decision Transformer paper evaluates results on v3 envs\n",
    "\n",
    "if args.env == 'walker2d':\n",
    "    env_name = 'Walker2d-v3'\n",
    "    rtg_target = 5000\n",
    "    env_d4rl_name = f'walker2d-{dataset}-v2'\n",
    "\n",
    "elif args.env == 'halfcheetah':\n",
    "    env_name = 'HalfCheetah-v3'\n",
    "    rtg_target = 6000\n",
    "    env_d4rl_name = f'halfcheetah-{dataset}-v2'\n",
    "\n",
    "elif args.env == 'hopper':\n",
    "    env_name = 'Hopper-v3'\n",
    "    rtg_target = 3600\n",
    "    env_d4rl_name = f'hopper-{dataset}-v2'\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "max_eval_ep_len = args.max_eval_ep_len  # max len of one episode\n",
    "num_eval_ep = args.num_eval_ep          # num of evaluation episodes\n",
    "\n",
    "batch_size = args.batch_size            # training batch size\n",
    "lr = args.lr                            # learning rate\n",
    "wt_decay = args.wt_decay                # weight decay\n",
    "warmup_steps = args.warmup_steps        # warmup steps for lr scheduler\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = args.max_train_iters\n",
    "num_updates_per_iter = args.num_updates_per_iter\n",
    "\n",
    "context_len = args.context_len      # K in decision transformer\n",
    "n_blocks = args.n_blocks            # num of transformer blocks\n",
    "embed_dim = args.embed_dim          # embedding (hidden) dim of transformer\n",
    "n_heads = args.n_heads              # num of transformer heads\n",
    "dropout_p = args.dropout_p          # dropout probability\n",
    "star_batch_size = args.star_batch_size \n",
    "\n",
    "# load data from this file\n",
    "dataset_path = f'{args.dataset_dir}/{env_d4rl_name}.pkl'\n",
    "\n",
    "# saves model and csv in this directory\n",
    "log_dir = args.log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# training and evaluation device\n",
    "device = torch.device(args.device)\n",
    "traj_dataset = Dataset(dataset_path, args.context_len,False)\n",
    "traj_test_dataset = TESTDataset(dataset_path, args.context_len,False)\n",
    "traj_star_dataset = D4RLTrajectoryDataset(dataset_path, context_len, rtg_scale)\n",
    "traj_data_loader = DataLoader(\n",
    "                        traj_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True,\n",
    "                        drop_last=True\n",
    "                    )\n",
    "traj_data_test_loader = DataLoader(\n",
    "                        traj_test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True,\n",
    "                        drop_last=True\n",
    "                    )\n",
    "traj_star_loader = DataLoader(\n",
    "                        traj_star_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True,\n",
    "                        drop_last=True\n",
    "                    )\n",
    "\n",
    "data_iter = iter(traj_data_loader)\n",
    "data_test_iter = iter(traj_data_test_loader)\n",
    "star_iter = iter(traj_star_loader)\n",
    "\n",
    "state_mean, state_std, reward_mean, reward_std, action_mean, action_std = traj_star_dataset.get_state_stats()\n",
    "\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "prefix = \"dt_\" + env_d4rl_name\n",
    "\n",
    "save_model_name =  prefix + \"_model_\" + start_time_str + \".pt\"\n",
    "save_model_path = os.path.join(log_dir, save_model_name)\n",
    "save_best_model_path = save_model_path[:-3] + \"_best.pt\"\n",
    "\n",
    "log_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
    "log_csv_path = os.path.join(log_dir, log_csv_name)\n",
    "\n",
    "csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
    "csv_header = ([\"duration\", \"num_updates\", \"action_loss\",\n",
    "                \"eval_avg_reward\", \"eval_avg_ep_len\", \"eval_d4rl_score\"])\n",
    "\n",
    "csv_writer.writerow(csv_header)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"start time: \" + start_time_str)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"device set to: \" + str(device))\n",
    "print(\"dataset path: \" + dataset_path)\n",
    "print(\"model save path: \" + save_model_path)\n",
    "print(\"log csv save path: \" + log_csv_path)\n",
    "\n",
    "\n",
    "# ## get state stats from dataset\n",
    "# state_mean, state_std = traj_dataset.get_state_stats()\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "model_guide = GuideVAE(args.feature_size, args.class_size, args.latent_size).to(device)\n",
    "optimizer_guide = torch.optim.AdamW(\n",
    "                    model_guide.parameters(),\n",
    "                    lr=lr,\n",
    "                    weight_decay=wt_decay\n",
    "                )\n",
    "scheduler_guide = torch.optim.lr_scheduler.LambdaLR(\n",
    "                        optimizer_guide,\n",
    "                        lambda steps: min((steps+1)/warmup_steps, 1)\n",
    "                )\n",
    "\n",
    "model = DecisionTransformer(\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            n_blocks=n_blocks,\n",
    "            h_dim=embed_dim,\n",
    "            context_len=context_len,\n",
    "            n_heads=n_heads,\n",
    "            drop_p=dropout_p,\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "                    model.parameters(),\n",
    "                    lr=lr,\n",
    "                    weight_decay=wt_decay\n",
    "                )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "                        optimizer,\n",
    "                        lambda steps: min((steps+1)/warmup_steps, 1)\n",
    "                    )\n",
    "\n",
    "max_d4rl_score = -1.0\n",
    "total_updates = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of updates: 20500\n",
      "GODA loss: 0.84079\n",
      "\n",
      "num of updates: 20600\n",
      "GODA loss: 0.82986\n",
      "\n",
      "num of updates: 20700\n",
      "GODA loss: 0.82824\n",
      "\n",
      "num of updates: 20800\n",
      "GODA loss: 0.81913\n",
      "\n",
      "num of updates: 20900\n",
      "GODA loss: 0.82884\n",
      "\n",
      "num of updates: 21000\n",
      "GODA loss: 0.82689\n",
      "\n",
      "num of updates: 21100\n",
      "GODA loss: 0.81664\n",
      "\n",
      "num of updates: 21200\n",
      "GODA loss: 0.81943\n",
      "\n",
      "num of updates: 21300\n",
      "GODA loss: 0.81405\n",
      "\n",
      "num of updates: 21400\n",
      "GODA loss: 0.81921\n",
      "\n",
      "num of updates: 21500\n",
      "GODA loss: 0.82731\n",
      "\n",
      "num of updates: 21600\n",
      "GODA loss: 0.82802\n",
      "\n",
      "num of updates: 21700\n",
      "GODA loss: 0.82215\n",
      "\n",
      "num of updates: 21800\n",
      "GODA loss: 0.83302\n",
      "\n",
      "num of updates: 21900\n",
      "GODA loss: 0.83071\n",
      "\n",
      "num of updates: 22000\n",
      "GODA loss: 0.83463\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m states \u001b[39m=\u001b[39m states\u001b[39m.\u001b[39mreshape(\u001b[39m64\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)          \u001b[39m# B x T x state_dim     [64,340]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mreshape(\u001b[39m64\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)        \u001b[39m# B x T x act_dim       [64,120]\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m rewards \u001b[39m=\u001b[39m rewards\u001b[39m.\u001b[39;49mreshape(\u001b[39m64\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)        \u001b[39m#                       [64,20]       \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#此处需要将三个变量合并起来\u001b[39;00m\n\u001b[1;32m     17\u001b[0m feature \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([states[:,\u001b[39m0\u001b[39m:\u001b[39mint\u001b[39m(state_dim\u001b[39m*\u001b[39m(context_len\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))],\n\u001b[1;32m     18\u001b[0m                         actions[:,\u001b[39m0\u001b[39m:\u001b[39mint\u001b[39m(act_dim\u001b[39m*\u001b[39m(context_len\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))],\n\u001b[1;32m     19\u001b[0m                         rewards[:,\u001b[39m0\u001b[39m:\u001b[39mint\u001b[39m((context_len\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))]],dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#s1a1r1~s5a5r5     #[64,240]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#先训好一个z*model：\n",
    "for i_iter in range(max_train_iters):\n",
    "    log_GODA_losses = []\n",
    "    if not args.eval:\n",
    "        for _ in range(num_updates_per_iter):\n",
    "            try:\n",
    "                states, actions, rewards = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(traj_data_loader)\n",
    "                states, actions, rewards = next(data_iter)\n",
    "\n",
    "            states = states.reshape(64,-1).to(device)          # B x T x state_dim     [64,340]\n",
    "            actions = actions.reshape(64,-1).to(device)        # B x T x act_dim       [64,120]\n",
    "            rewards = rewards.reshape(64,-1).to(device)        #                       [64,20]       \n",
    "            \n",
    "            #此处需要将三个变量合并起来\n",
    "            feature = torch.cat([states[:,0:int(state_dim*(context_len/2))],\n",
    "                                    actions[:,0:int(act_dim*(context_len/2))],\n",
    "                                    rewards[:,0:int((context_len/2))]],dim=1) #s1a1r1~s5a5r5     #[64,240]\n",
    "            feature_class = torch.cat([states[:,int(state_dim*(context_len/2)):state_dim*context_len],\n",
    "                                        actions[:,int(act_dim*(context_len/2)):act_dim*context_len],\n",
    "                                        rewards[:,int((context_len/2)):context_len]],dim=1) #s6a6r6~s10a10r10      [64,240]                                     #\n",
    "\n",
    "            recon_mu, recon_std, z1_mu, z1_log_std = model_guide.forward(feature, feature_class)\n",
    "            z2_mu, z2_log_std, recon_mu2, recon_log_std2 = model_guide.reconstruct(feature)\n",
    "            GODA_loss = model_guide.loss_function(recon_mu, recon_std, feature_class, z1_mu, z1_log_std, z2_mu, z2_log_std)\n",
    "\n",
    "            optimizer_guide.zero_grad()\n",
    "            GODA_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model_guide.parameters(), 0.25)\n",
    "            optimizer_guide.step()\n",
    "            scheduler_guide.step()\n",
    "\n",
    "            log_GODA_losses.append(GODA_loss.detach().cpu().item())\n",
    "    mean_GODA_loss = np.mean(log_GODA_losses)\n",
    "    total_updates += num_updates_per_iter\n",
    "    log_str = (\n",
    "        \"num of updates: \" + str(total_updates) + '\\n' +\n",
    "        \"GODA loss: \" +  format(mean_GODA_loss, \".5f\") + '\\n'\n",
    "    )\n",
    "    print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_GODA_loss=[]\n",
    "for _ in range(num_updates_per_iter):\n",
    "    try:\n",
    "        states, actions, rewards = next(data_test_iter)\n",
    "    except StopIteration:\n",
    "        data_test_iter = iter(traj_data_test_loader)\n",
    "        states, actions, rewards = next(data_test_iter)\n",
    "\n",
    "    states = states.reshape(64,-1).to(device)          # B x T x state_dim     [64,170]\n",
    "    actions = actions.reshape(64,-1).to(device)        # B x T x act_dim       [64,60]\n",
    "    rewards = rewards.reshape(64,-1).to(device)  \n",
    "\n",
    "    feature = torch.cat([states[:,0:int(state_dim*(context_len/2))],\n",
    "                                actions[:,0:int(act_dim*(context_len/2))],\n",
    "                                rewards[:,0:int((context_len/2))]],dim=1) #s1a1r1~s5a5r5     \n",
    "                                                                    #\n",
    "    feature_class = torch.cat([states[:,int(state_dim*(context_len/2)):state_dim*context_len],\n",
    "                                actions[:,int(act_dim*(context_len/2)):act_dim*context_len],\n",
    "                                rewards[:,int((context_len/2)):context_len]],dim=1) #s6a6r6~s10a10r10                                                 #\n",
    "        \n",
    "    recon_mu, recon_std, z1_mu, z1_log_std = model_guide.forward(feature, feature_class)\n",
    "    z2_mu, z2_log_std, recon_mu2, recon_log_std2 = model_guide.reconstruct(feature)\n",
    "    GODA_loss = model_guide.loss_function(recon_mu, recon_std, feature_class, z1_mu, z1_log_std, z2_mu, z2_log_std)\n",
    "\n",
    "    final_GODA_loss.append(GODA_loss.detach().cpu().item())\n",
    "mean_GODA_final_loss = np.mean(final_GODA_loss)\n",
    "log_str = (\n",
    "        \"GODA test loss: \" +  format(mean_GODA_final_loss, \".5f\") + '\\n'\n",
    "    )\n",
    "print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "time elapsed: 1:21:21\n",
      "num of updates: 22100\n",
      "action loss: 0.65598\n",
      "eval avg reward: -220.21380\n",
      "eval avg ep len: 1000.00000\n",
      "eval d4rl score: 0.48300\n",
      "max d4rl score: -1.00000\n",
      "saving max d4rl score model at: dt_runs/dt_halfcheetah-medium-v2_model_23-03-13-14-34-44_best.pt\n",
      "saving current model at: dt_runs/dt_halfcheetah-medium-v2_model_23-03-13-14-34-44.pt\n",
      "============================================================\n",
      "time elapsed: 1:22:50\n",
      "num of updates: 22200\n",
      "action loss: 0.60643\n",
      "eval avg reward: -190.82839\n",
      "eval avg ep len: 1000.00000\n",
      "eval d4rl score: 0.71969\n",
      "max d4rl score: 0.48300\n",
      "saving max d4rl score model at: dt_runs/dt_halfcheetah-medium-v2_model_23-03-13-14-34-44_best.pt\n",
      "saving current model at: dt_runs/dt_halfcheetah-medium-v2_model_23-03-13-14-34-44.pt\n"
     ]
    }
   ],
   "source": [
    "def goback(rtg, r, r_, r_mean, r_std, rtg_scale):\n",
    "    r = r*r_std+r_mean\n",
    "    r_ = r_*r_std+r_mean\n",
    "    rtg = rtg*rtg_scale\n",
    "    rtg_ = np.zeros_like(rtg)\n",
    "    rtg = rtg.to(torch.device('cuda:0'))\n",
    "    rtg_ = torch.from_numpy(rtg_).to(torch.device('cuda:0'))\n",
    "    for i in range(20, 10, -1):\n",
    "        temp = torch.zeros(64).to(torch.device('cuda:0'))\n",
    "        for j in range(i - 11, 10):\n",
    "            temp += r_[:,j] - r[:,j]\n",
    "        rtg_[:,i-1] = rtg[:,i-1] + temp\n",
    "    return rtg_[:,10:20]/rtg_scale\n",
    "\n",
    "for i_train_iter in range(max_train_iters):\n",
    "\n",
    "    log_action_losses = []\n",
    "    model.train()\n",
    "\n",
    "    for _ in range(num_updates_per_iter):\n",
    "        try:\n",
    "            timesteps, states, actions, returns_to_go, traj_mask, rewards = next(star_iter)\n",
    "        except StopIteration:\n",
    "            star_iter = iter(traj_star_loader)\n",
    "            timesteps, states, actions, returns_to_go, traj_mask, rewards = next(star_iter)\n",
    "\n",
    "        #将采样出的数据输入增强网络********************************************************************************************************************************\n",
    "        states = states.reshape(star_batch_size ,-1).to(device)          # B x T x state_dim     [64,170]                                                       # \n",
    "        actions = actions.reshape(star_batch_size ,-1).to(device)        # B x T x act_dim       [64,60]                                                        #\n",
    "        rewards = rewards.reshape(star_batch_size ,-1).to(device)        #                       [64,10]                                                        #\n",
    "                                                                                                                                                                \n",
    "        feature = torch.cat([states[:,0:int(state_dim*(context_len/2))],  \n",
    "                                actions[:,0:int(act_dim*(context_len/2))],                                                                                      #\n",
    "                                    rewards[:,0:int((context_len/2))]],dim=1) #s1a1r1~s5a5r5                                                                       #    \n",
    "\n",
    "        feature_class = torch.cat([states[:,int(state_dim*(context_len/2)):state_dim*context_len],                                                              #\n",
    "                                    actions[:,int(act_dim*(context_len/2)):act_dim*context_len],                                                                #\n",
    "                                    rewards[:,int((context_len/2)):context_len]],dim=1) #s6a6r6~s10a10r10                                                       #\n",
    "                                                                                                                                                                \n",
    "        z1_mu, z1_log_std = model_guide.encode(feature, feature_class, True)                                                                                    #\n",
    "        z2_mu, z2_log_std = model_guide.encode(feature, 0, False)                                                                                               #\n",
    "        z2_log_std = torch.clamp(z2_log_std, -20, 2)                                                                                                            #\n",
    "        z2_std = torch.exp(z2_log_std)                                                                                                                          #\n",
    "\n",
    "        # recon_mu_ori,recon_log_std_ori = model_guide.decode(z1_mu,feature)                                                                                    #\n",
    "        z = z1_mu.clone().detach()                                                                                                                              #\n",
    "        z.requires_grad = True                                                                                                                                  #\n",
    "        scene_optim = torch.optim.Adam([z], lr=lr)                                                                                                              #\n",
    "        loss_star_function = Starloss()                                                                                                                         #\n",
    "        loss =[]                                                                                                                                                #\n",
    "        \n",
    "        #找到这64条序列对应的最好的z                                                                                                                              #                \n",
    "        for i in range(200):                                                                                                                                    #\n",
    "            recon_mu,recon_log_std = model_guide.decode(z,feature)                                                                                              #\n",
    "            rewards_ = torch.mean(recon_mu[:,(state_dim+act_dim)*int(context_len/2):])#rewards: s6'~s10'                                                        #\n",
    "            loss_star = loss_star_function.forward(rewards_, z, z1_mu, z2_mu, z2_std)                                                                           #\n",
    "            loss.append(loss_star.detach().cpu().item())                                                                                                        #\n",
    "            scene_optim.zero_grad()                                                                                                                             #                 \n",
    "            loss_star.backward(retain_graph=True)                                                                                                               #\n",
    "            scene_optim.step()                                                                                                                                  #\n",
    "        \n",
    "    #替换数据                                                                                                                                                    #\n",
    "        #恢复s6'a6'r6'~s10'a10'r10'                                                                                                                             #\n",
    "        timesteps = timesteps.to(device)    # B x T                                                                                                             #\n",
    "        states_ = recon_mu[:,0:int(state_dim*(context_len/2))]                                                                                               #\n",
    "        actions_ = recon_mu[:,state_dim*int(context_len/2):(state_dim+act_dim)*int(context_len/2)].cpu().detach().numpy()\n",
    "        actions_ =  (actions_.reshape(64,10,6)*action_std+action_mean).reshape(64,-1)\n",
    "        rewards_ = recon_mu[:,(state_dim+act_dim)*int(context_len/2):]\n",
    "        returns_to_go_ = goback(    #[64,10]\n",
    "            returns_to_go,   #[64,20]\n",
    "            rewards[:,int((context_len/2)):context_len],   #[64,10]\n",
    "            rewards_,   #[64,10]\n",
    "            reward_mean, \n",
    "            reward_std,\n",
    "            rtg_scale).to(device)  \n",
    "        \n",
    "        #拼凑1~5+6'~10'\n",
    "        states = torch.cat([states[:,0:int(state_dim*(context_len/2))],\n",
    "                            states_]).reshape(star_batch_size, context_len, state_dim).reshape(64,20,17).to(device)\n",
    "        actions = torch.cat([actions[:,0:int(act_dim*(context_len/2))],\n",
    "                                torch.from_numpy(actions_).to(device)]).reshape(star_batch_size, context_len, act_dim).reshape(64,20,6).to(device) \n",
    "        \n",
    "        returns_to_go = torch.cat([returns_to_go[:,0:int((context_len/2))].to(device),\n",
    "                                    returns_to_go_]).reshape(star_batch_size , context_len, 1).to(device)  \n",
    "                    \n",
    "        traj_mask = traj_mask.to(device)    # B x T                                                                                                             #\n",
    "        action_target = torch.clone(actions).detach().to(device)                                                                                                #\n",
    "        #********************************************************************************************************************************************************\n",
    "        #用star数据去训练\n",
    "        state_preds, action_preds, return_preds = model.forward(\n",
    "                                                        timesteps=timesteps,\n",
    "                                                        states=states,\n",
    "                                                        actions=actions,\n",
    "                                                        returns_to_go=returns_to_go\n",
    "                                                    )\n",
    "        # only consider non padded elements\n",
    "        action_preds = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
    "        action_target = action_target.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
    "\n",
    "        action_loss = F.mse_loss(action_preds, action_target, reduction='mean')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        log_action_losses.append(action_loss.detach().cpu().item())\n",
    "\n",
    "    # evaluate action accuracy\n",
    "    results = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
    "                            num_eval_ep, max_eval_ep_len, state_mean, state_std)\n",
    "\n",
    "    eval_avg_reward = results['eval/avg_reward']\n",
    "    eval_avg_ep_len = results['eval/avg_ep_len']\n",
    "    eval_d4rl_score = get_d4rl_normalized_score(results['eval/avg_reward'], env_name) * 100\n",
    "\n",
    "    mean_action_loss = np.mean(log_action_losses)\n",
    "    time_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
    "\n",
    "    total_updates += num_updates_per_iter\n",
    "\n",
    "    log_str = (\"=\" * 60 + '\\n' +\n",
    "            \"time elapsed: \" + time_elapsed  + '\\n' +\n",
    "            \"num of updates: \" + str(total_updates) + '\\n' +\n",
    "            \"action loss: \" +  format(mean_action_loss, \".5f\") + '\\n' +\n",
    "            \"eval avg reward: \" + format(eval_avg_reward, \".5f\") + '\\n' +\n",
    "            \"eval avg ep len: \" + format(eval_avg_ep_len, \".5f\") + '\\n' +\n",
    "            \"eval d4rl score: \" + format(eval_d4rl_score, \".5f\")\n",
    "        )\n",
    "\n",
    "    print(log_str)\n",
    "\n",
    "    log_data = [time_elapsed, total_updates, mean_action_loss,\n",
    "                eval_avg_reward, eval_avg_ep_len,\n",
    "                eval_d4rl_score]\n",
    "\n",
    "    csv_writer.writerow(log_data)\n",
    "\n",
    "    # save model\n",
    "    print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "    if eval_d4rl_score >= max_d4rl_score:\n",
    "        print(\"saving max d4rl score model at: \" + save_best_model_path)\n",
    "        torch.save(model.state_dict(), save_best_model_path)\n",
    "        max_d4rl_score = eval_d4rl_score\n",
    "\n",
    "    print(\"saving current model at: \" + save_model_path)\n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"finished training!\")\n",
    "print(\"=\" * 60)\n",
    "end_time = datetime.now().replace(microsecond=0) \n",
    "time_elapsed = str(end_time - start_time)\n",
    "end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "print(\"started training at: \" + start_time_str)\n",
    "print(\"finished training at: \" + end_time_str)\n",
    "print(\"total training time: \" + time_elapsed)\n",
    "print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "print(\"saved max d4rl score model at: \" + save_best_model_path)\n",
    "print(\"saved last updated model at: \" + save_model_path)\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safe-slac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
